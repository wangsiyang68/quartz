•        **Week 1 (Matt):** Introduction to course, some ML jargon reminders, linear and polynomial regression, generalization, ridge regression and regularization, overfitting/underfitting, logistic regression, neural networks and how they relate to biology. Building our own custom class of shallow neural networks, along with [[Gradient Problems|gradient descent]], training and testing procedure.
- Other basics: [[Activation Functions]], [[Batch Size]], Parameter initialization

•        **Week 2 (Matt):** Introduction to [[PyTorch]] framework, tensors and dataloaders objects. Implementing a shallow neural network in PyTorch, backpropagation in PyTorch with AutoGrad, [[Optimizers|advanced optimizers]], multi-label classification with shallow neural networks. Finally, moving from shallow to deep neural networks.

•        **Week 3 (Matt):** Guided project and good practices for Deep Learning projects (train/test/dev, bias/variance, advanced regularization, dropout, normalizing inputs/outputs/layers, trainer functions, savers/loader functions for reproducibility and transfer learning). 
- Other basics: Evaluating models using [[Kullback-Leibler Divergence|KL Divergence]] and [[Multi Class Cross Entropy]]

•        **Week 4 (Man):** The image data type, image processing techniques and typical computer vision operations, the convolution operation and layers ([[CV specific tricks]]), Convolutional Neural Networks, advanced CNNs and SOTA. Preparing transition to the 50.035 Computer Vision course. Adversarial machine learning, [[DL Attacks|attacking a Neural Network]] with basic gradient-based attacks, fundamental limits of Neural Networks, defence mechanisms and state-of-the-art of some advanced attacks techniques.

•        **Week 5 (Man):** Continuation of Week 4.

•        **Week 6 (Matt):** [[Time Series Problem|Sequential data]] (times series, text, etc.), vanilla [[Recurrent Neural Network]]s, [[GRU|Gated Recurrent Units]], [[LSTM|Long-Short Term Memory]] cells, advanced RNN networks, mixing models for advanced architectures.  

•        **Week 7 (NA):** Recess week.

•        **Week 8 (Matt):** The embedding problem, more advanced concepts on RNNs, introduction to Natural Language Processing (NLP) and Word Embeddings for NLP, brief state-of-the-art on NLP, attention and transformers architectures.  
Preparing transition to the 50.040 Natural Language Processing course.

•        **Week 9 (Man):** Quick introduction to Graph Theory and typical graph datasets and problems, basics of Graph Convolutional Networks, brief state-of-the-art of advanced Graph Convolutional Networks.

•        **Week 10 (Man):** [[Generative Models]], Autoencoders and Variational Autoencoders, Generative Adversarial Networks (GANs), Advanced concepts on Generative Adversarial Networks, Practice on GANs.

•        **Week 11 (Man):** Topics for curiosity. Introduction to Physics-Informed Neural Networks. Introduction to diffusion models. Introduction to Explainability/Interpretability and open questions in research about Neural Networks. What will be the next revolution in AI? (a word on ChatGPT, Dall-E, etc.).

•        **Week 12 (Matt):** Brief introduction to reinforcement learning, and state-action-rewards systems, multi-armed bandit problem and the exploration/exploitation trade-off, Q-learning and Deep Q-Learning. Brief state-of-the-art discussion about further works in Reinforcement Learning and the recent uses of Reinforcement Learning with Human Feedback.

•        **Week 13 (Man and Matt):** Recap. Closing and future directions for studying Deep Learning. Project presentations and guest conferences (TBA).

#dl