- **Week 1 (Matt):** **Introduction to course**
	- Some ML jargon reminders, 
	- Linear and polynomial regression
	- Generalization
	- Ridge regression and regularization, 
	- Overfitting/underfitting, 
	- Logistic regression
	- Neural networks and how they relate to biology. 
	- Building our own custom class of shallow neural networks, along with [[Gradient Problems|gradient descent]], training and testing procedure.
	- [[Activation Functions]]
	- [[Batch Size]]
	- Parameter initialization

- **Week 2 (Matt):** **Introduction to [[PyTorch]] framework**, 
	- Tensors
	- Dataloaders objects
	- Implementing a shallow neural network in PyTorch
	- Backpropagation in PyTorch with AutoGrad, 
	- [[Optimizers|Advanced Optimizers]] 
	- multi-label classification with shallow neural networks. 
	- Finally, moving from shallow to deep neural networks.

- **Week 3 (Matt):** **Good practices for Deep Learning projects** 
	- train/test/dev
	- bias/variance
	- advanced regularization
	- dropout
	- normalizing inputs/outputs/layers
	- trainer functions
	- savers/loader functions for reproducibility and transfer learning
	- Evaluating models using 
		- [[Kullback-Leibler Divergence|KL Divergence]] 
		- [[Multi Class Cross Entropy]]

- **Week 4 (Man):** **Image-data based tasks**
	- The image data type
	- Image processing techniques and typical computer vision operations
	- Convolution operation and layers ([[CV specific tricks]]), 
	- Convolutional Neural Networks
	- Advanced CNNs and SOTA.
	- [[DL Attacks|Attacking ML models]]
		- Basic gradient-based attacks
		- Fundamental limits of Neural Networks
		- Defence mechanisms and state-of-the-art of some advanced attacks techniques.

- **Week 5 (Man):** Continuation of Week 4.

- **Week 6 (Matt):** **Sequential-data based tasks**
	- [[Time Series Problem|Sequential data]] (times series, text, etc.)
	- Vanilla [[Recurrent Neural Network]]s 
	- [[GRU|Gated Recurrent Units]]
	- [[LSTM|Long-Short Term Memory]] cells
	- Advanced RNN networks
	- Mixing models for advanced architectures.  

- **Week 7 (NA):** Recess week.

- **Week 8 (Matt):** **Introduction to embeddings & Transformers**
	- The embedding problem, 
	- More advanced concepts on RNNs
	- Encoder Decoder Architecture
	- Introduction to Natural Language Processing (NLP) 
	- Word Embeddings for NLP
	- Brief state-of-the-art on NLP
	- [[Attention]] and transformers architectures.  

- **Week 9 (Man):** **[[Vision Transformers]]**
	- Layer Normalization
	- Multi Head Self Attention
	- Positional Encoding
	- ViT issues
	- Other advanced ViTs
	- Neural Architecture Search
	- Kendall Rank Coefficient

- **Week 10 (Man):** **[[Generative Models]]** 
	- Autoencoders and Variational Autoencoders
	- [[Generative Adversarial Networks]] (GANs)
	- Minimising Jensen-Shannon divergence
	- Advanced concepts on Generative Adversarial Networks
	- GAN issues

- **Week 11 (Man):** **More about GANs**
	- Different GANs
		- Wasserstein GAN
		- StyleGAN
		- CycleGAN
	- Latent diffusion model

- **Week 12 (Matt):** **Introduction to [[Reinforcement Learning]]**
	- State-action-rewards systems
	- Multi-armed bandit problem
	- The exploration/exploitation trade-off 
	- Q-learning and Deep Q-Learning
	- Actor-critic learning methods
	- Brief state-of-the-art discussion about further works in Reinforcement Learning 
	- Recent uses of Reinforcement Learning with Human Feedback
	- Other Problems
		- [[Physics-Informed Neural Networks]]
		- Markov states
		- Partially observalbe environment
		- SARSA
		- Non-stationary problems

#dl